<link rel="stylesheet"
      href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script >hljs.initHighlightingOnLoad();</script>

<link rel="stylesheet" href="./js/bootstrap.min.css"
      integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
<script src="./js/jquery-3.2.1.slim.min.js"
        integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
        crossorigin="anonymous"></script>
<script src="./js/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q"
        crossorigin="anonymous"></script>
<script src="./js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl"
        crossorigin="anonymous"></script>
<script src="./js/Chart.bundle.min.js"></script>
<link rel="stylesheet" href="./js/style.css">


<script>
    var colors = ['rgb(255, 99, 132)', 'rgb(75, 192, 192)', 'rgb(54, 162, 235)', 'rgb(255, 159, 64)', 'rgb(201, 203, 207)']
    var data = {
    "mapper_cost": {"labels":["Full Table Scan", "C.P. on, Full Table Scan"],"datasets":[{"label":"RDD scan mapper","data":[34499,21218]},{"label":"RDD scan","data":[14282,6479]},{"label":"DF scan","data":[13276,7708]},{"label":"DS scan","data":[18122,12544]}]},

    "join_cardinality": {"labels":["user => purchase","C.P. on, user => purchase","purchase => user","C.P. on, purchase => user"],"datasets":[
            {"label":"joinWithCassandra","data":[25394,24888,94676,94195]},
            {"label":"joinWithCassandra Mapper","data":[20878,21621,94516,96470]},
            {"label":"RDD join keyBy","data":[30191,23422,28168,22721]},
            {"label":"RDD join keyBy same partitioners","data":[13660,7676,13519,7370]},
            {"label":"RDD join span by key","data":[23222,16909,23027,16260]},
            {"label":"Spark Dataset Join","data":[23612,17842,17337,10659]},
            {"label":"Spark Dataframe Join","data":[17372,11330,16553,10110]}]}
        ,
        "join_cardinality_10": {"labels":["user (10%) => purchase","C.P. on, user (10%) => purchase","purchase => user (10%)","C.P. on, purchase => user (10%)"],"datasets":[
                {"label":"joinWithCassandra","data":[2332,2296,72484,75793]},
                {"label":"joinWithCassandra Mapper","data":[2486,2461,73091,75844]},
                {"label":"RDD join keyBy","data":[26123,19948,26634,20140]},
                {"label":"RDD join keyBy same partitioners","data":[11443,6783,11248,6221]},
                {"label":"RDD join span by key","data":[21863,14959,21501,14929]},
                {"label":"Spark Dataset Join","data":[16630,9876,15098,9343]},
                {"label":"Spark Dataframe Join","data":[16318,9219,15099,8724]}]},

        "broadcast_join": {"labels":["user => shop Broadcast","user => shop"],"datasets":[{"label":"Spark small Dataframe join","data":[5814,11443]},{"label":"Spark small RDD join","data":[2906,19692]}]},
        "broadcast_join_10": {"labels":["user (10%) => shop Broadcast","user (10%) => shop"],"datasets":[{"label":"Spark small Dataframe join","data":[2547,1113]},{"label":"Spark small RDD join","data":[836,2054]}]},
        "space_efficiency": {"labels":["Purchase table size in memory (MB)"],"datasets":[{"label":"RDD","data":[1110.7]},{"label":"Dataframe","data":[231.8]}]},
        "rdd_dataframe": {"labels":["max purchase per item","sum (max purchase per user)","to lowercase","UDF concat", "distinct"],"datasets":[{"label":"RDD","data":[343317,329204,325021,342299,334273]},{"label":"dataframe","data":[42359,18238,21176,56578,24630]},{"label":"Dataset","data":[0,47995,17349,33151,36614]}]}

    }

    window.onload = function () {
        jQuery("._canvas").each(function (i) {
            var ctx = this.getContext("2d");
            var d = data[$(this).attr("data-name")]
            $.each(d.datasets, function(i) {
                d.datasets[i].backgroundColor = colors[i];
            })
            var chart = new Chart(ctx, {
                type: 'bar',
                data: d,
                options: {
                    responsive: true,
                    legend: {
                        position: 'top',
                    },
                    title: {
                        display: true,
                        text: 'Chart.js Bar Chart'
                    },
                    scales: {
                        yAxes: [{
                            ticks: {
                                beginAtZero: true
                            }
                        }]
                    }
                }
            });
        })
    };
</script>

<div class="container">
    <h1>Spark guide</h1>
    DSE Analytics main components you should know know before starting:
    <ul>
        <li>Master</li>
        <li>Workers</li>
        <li>Drivers</li>
        <li>DSEFS</li>
        <li>Executors</li>
        <li>DSE process (Cassandra)</li>
        <li>Shuffle service (extra)</li>
        <li>Job history (extra)</li>
        <li>Job server (extra)</li>
    </ul>



    <h2>Spark operations</h2>
    <h3>Resources</h3>
    By default, spark will launch new jobs using all cluster resources. <br/>
    It's often a best practice to reduce the default configuration of the number of core and memory used to avoid having a long running job (spark shell or sql) stealing all resources.<br/>
    Finding the best amount of memory and cores for a job isn't always easy. The following can help define a good default value.<br/>
    <ul>
        <li>Having many core will likely speed-up the computation. Start with spark.cores.max = 3-4 x number executor</li>
        <li>Having more memory will allow bigger partition. 4-8GB is a good start (see details below).</li>
        <li>The executor memory is split between all its core. Having 1 executor running 4 cores on a 1GB heap is likely wrong.</li>
        <li>It's generally best to define a number of core being a multiple of your number of executor</li>
        <li>The number of core per executor is computed by totalNumberOfCore / numberOfExecutor </li>
        <li><a href="https://github.com/groupon/sparklint">Sparklint</a> can be useful to optimize cluster utilization</li>
    </ul>
    <h3>Running multiple spark application: dynamic resource allocation</h3>
    Dynamic allocation can be used to allow a spark job to dynamically extends its resources (request more executors).
    see <a href="https://spark.apache.org/docs/latest/job-scheduling.html#configuration-and-setup">Spark documentation detail</a>
    (typically a long running job with small periodic jobs)
    <h3>Running jobs within a single spark application: fair scheduler</h3>
    By default, spark will execute jobs in a FIFO order.<br/>
    If multiple jobs need to be running at the same time, the fair scheduler can be used to run multiple jobs in parrallel (multiple pools can be defined, each pools can be FIFO or FAIR).<br/>
    see <a href="https://spark.apache.org/docs/latest/job-scheduling.html#configuration-and-setup">Spark documentation detail</a>

    <h2>Running your job</h2>
    <h3>How to submit</h3>
    There are mainly 4 ways to submit a spark job:
    <ul>
        <li>from a cluster node, using dse submit</li>
        <li>Spark http api (not supported, not working in DSE > 5.1. https://gist.github.com/arturmkrtchyan/5d8559b2911ac951d34a)</li>
        <li>cql submission (experimental, not supported)</li>
        <li>Spark jobserver (not HA)</li>
    </ul>

    It's easier to submit jars to DSEFS, especially when running on cluster mode.
    <pre><code>
    //send jar to dsefs. DSEFS HTTP api can also be used.
    dse fs "put /home/quentin/testspark-0.0.1-SNAPSHOT.jar /testspark-0.0.1-SNAPSHOT.jar"
    //start job
    dse spark-submit --class TestSpark --files /home/quentin/logback-spark-test.xml --conf "spark.driver.extraJavaOptions=-Dlogback.configurationFile=logback-spark-test.xml" --conf "spark.executor.extraJavaOptions=-Dlogback.configurationFile=logback-spark-test.xml" --deploy-mode cluster dsefs://localhost:5598/testspark-0.0.1-SNAPSHOT.jar
    </code></pre>
    <!-- //TODO <h3>Configuration files</h3>-->

    <h3>Monitoring apis</h3>
    <ul>
        <li>Master api: http://masterUI:7080/json</li>
        <li>Standar api: https://spark.apache.org/docs/latest/monitoring.html</li>
    </ul>


    <h2>Memory & partition size</h2>
    <h3>Memory</h3>
    By default, spark starts its executors with 1GB of memory. That's also what you get when you start a spark shell of a sparkSQL with dse sql.<br/>
    Over those 1GB, 300MB are allocated to spark.<br/>
    Over the 700MB remaining (1GB-300MB), 60% (by default) are reserved to spark execution (execution and persisted data).<br/>
    That left roughly 300MB of memory for user. If you use more than this amount of memory in 1 spark partition (task), you'll get an OOM exception.<br/>
    <a href="http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/">Cloudera "How-to: Tune Your Apache Spark Jobs"</a>

    <h3>Partition number</h3>
    <h4>Understanding spark partitioning</h4>
    <a href="https://techmagie.wordpress.com/2015/12/19/understanding-spark-partitioning/">Make sure you read this blog post on Spark Partitioning</a>
    <h4>Dataframe</h4>
    The following <strong>global</strong> parameter configures the number of partitions that are used when a shuffle is performed over sparkSQL/Dataframe (joins, aggregations...).<br/>
    In addition, spark nows includes a .repartition() method that can be used if necessary.<br/>
    <pre><code>
    spark.sql.shuffle.partitions
    </code></pre>
    <div class="alert alert-info" role="alert">
    This setting can't be changed for a specific operation. Spark will use (max(spark.sql.shuffle.partitions, actualDataframePartitionNumber)) partitions during its join operations. <br/>
    If you know that your operation will require a lot of partitions, repartition your DF before or use smaller cassandra split size.
    </div>

    <h4>RDD</h4>
    The same parameter is available for RDDs on operations like join, reduceByKey, and parallelize (when not set explicitly by the user).<br/>
    <pre><code>
    spark.default.parallelism
    </code></pre>
    <h4>How many partitions?</h4>
    The number of partition is generally driver by the data size. As a good default, try to aim for a partition size of 128MB (128MB = how many data is handled by a partition in memory).<br/>
    <strong>Most of the jobs use from 200 to 2000 partition.</strong><br/>
    When tuning the number of partitions, consider the following objectives:<br/>
    <ul>
        <li>Avoid OOM</li>
        <li>Avoid GC saturation (the webui show you the GC time of each task)</li>
        <li>Avoid spilling to disk</li>
        <li>Avoid many small operations</li>
    </ul>
    Few partitions will result to no/bad parallelism, too many add overhead.<br/>
    Here is a formula to get a rought idea of the ideal number of partition. <br/>
    size.of.stage can be find by saving the data and checking its size in the webui.<br/>
    <pre><code>
    //If data is being cached:
    memory.available.per.task = (spark.executor.memory - overhead) x spark.memory.fraction x (1 - spark.memory.storageFraction) / spark.executor.cores
    partition.number = size.of.stage / memory.available.per.task
                     = size.of.stage / ((spark.executor.memory - overhead) x spark.memory.fraction x (1 - spark.memory.storageFraction) / spark.executor.cores)
                     = size.of.stage / ((spark.executor.memory - 300) x 0.6 x (1 - 0.5) / spark.executor.cores)
                     = size.of.stage / ((spark.executor.memory - 300) x 0.3 / spark.executor.cores)
    //If no data is being cached, spark.memory.storageFraction = 0, so in this case:
    partition.number = size.of.stage / ((spark.executor.memory - 300) x 0.6 / spark.executor.cores)
    </code></pre>
    <div class="alert alert-info" role="alert">
        Another approximation for the size.of.stage could be the following : shuffle write * Shuffle spill (memory) / Shuffle spill (disk).<br/>
        Shuffle write can be find in the stage detail.<br/>
        from <a href="http://jason4zhu.blogspot.fr/2015/06/roaming-through-spark-ui-and-tune-performance-upon-a-specific-use-case.html">Jason blog post</a>:
        FYI: Shuffle spill (memory) is the size of the deserialized form of the data in memory at the time when we spill it, whereas shuffle spill (disk) is the size of the serialized form of the data on disk after we spill it<br/>
    </div>
    <div class="alert alert-info" role="alert">
        Skew data is painful and are likely to create big partitions. Salt technique are generally use to mitigate this issue.<br/>
    </div>
    <div class="alert alert-info" role="alert">
        Note that Spark handles partitions differently when there is more than 2000 partition. If your partition number is close to this number, try to increase it to more than 2000.
    </div>
    <div class="alert alert-primary" role="alert">
        The previous formula is quite theorical and isn't always required. The following strategy can be used instead:
        <ul>
            <li>Start with "decent" default executor memory (4-8g)</li>
            <li>having OOM or lot of GC time, data is spilling to disk ? =>
                <ul>
                    <li>Increase memory</li>
                    <li>Increase number of partition</li>
                    <li>Reduce C* split size (spark.cassandra.input.split.size_in_mb, see below)</li>
                </ul>
            </li>
            <li>Repeat</li>
        </ul>
    </div>
    <a href="http://shop.oreilly.com/product/0636920046967.do">Spark perf tuning</a> from Holden Karau and Rachel Warren is a good reference.


    <h3>Cassandra split size</h3>
    While reading Cassandra table, the connector will try to split the data by spark partition off a given size. <br/>
    1 spark partition contains N Cassandra partitions. however 1 Cassandra partition will always be loaded in a unique spark partition. <br/>
    Spark partition size are controlled by the following parameter:
    <pre><code>
    spark.cassandra.input.split.size_in_mb = 128
    </code></pre>
    <div class="alert alert-info" role="alert">
    Note: DSE 5.1 increase this value to 512mb by default for you.
    </div>
    Keep in mind that this setting is just an estimate based on Cassandra table size estimation. To have a better idea of your partition size, check the in/out parameter of each task in the spark webui.<br/>
    If your spark application is running out of memory, increase spark executor heap and/or lower the original split size/<br/>
    This value define the partition size when reading data from Cassandra only, it doesn't apply for joins operation.

    <h3>Repartition</h3>
    Repartition is generally used to increase the number of partition of your RDD or Dataframe. It's an expensive operation since it will require to shuffle all data. <br/>
    Avoid unless necessary. It's generally a better practice to lower the original partition size. <br/>
    It's generally best to repartition to a number being a multiple of your total number of core.

    <h3>Coaelesce</h3>
    Coaelesce reduce the number of partition in a RDD or Dataframe. Unless the shuffle flag is set to true, it's a simple operation, partitions will just be merged on each executors.<br/>

    <h2>RDD vs DataFrame</h2>
    Dataframe is spark new shiny api. Dataframes use Catalyst, a query optimizer with code generations.<br/>
    Spark analyse your operations and builds a tree containing all your stepts, generating code on the fly. It also allows spark to be smart and re-order operations if needed (filter will be applied before joins etc.)<br/>
    RDD are a now a lower level of abstraction and should be used only when Dataframe can't be used, or in specific case like joins (see below, will change in future versions).<br/>

    <h3>Serialization</h3>
    Java Objects consume a lot of memory. Typically, a String will use:
    <ul>
        <li>8(mark word) + 4(klass pointer) = 12 bytes for the header</li>
        <li>8 bytes for the hashCode</li>
        <li>8 bytes for the char[] array reference</li>
        <li>4 bytes for the char[] array lenght</li>
        <li>and finally 2 bytes per char in the string</li>
    </ul>
    <h4>Dataframe</h4>
    To solve this issue, Dataframes use Tungsten, a serialization engine designed to store binary data. It allows byte-level comparison, preventing costly object creation.<br/>
    All operations manipulating columns will beneficiate from Tungsten. Transformation/Closure operations (.map(...), .filter(row => ...) etc.) will trigger an expensive Ser/Deser step.
    <h4>Typed Dataset</h4>
    Since spark 2.0, default builtin codec are available for case classes, making the use of Dataset (typed api) easier.<br/>
    However, this api will have to serialize/deserialize Java object, loosing Dataframe ability to mutate column in a very efficient way.<br/>
    Custom encoders can be built to provide serialization for non-supported type.<br/>
    <h4>RDD</h4>
    When RDD are used, serialization speed can be improved <a href="http://spark.apache.org/docs/latest/tuning.html#data-serialization">using Kryo format</a>. <br/>

    <h3>Space efficiency</h3>
    Here is a simple example of RDD vs Dataframe space efficiency, simply storing data in memory:
    <pre><code>
    spark.sparkContext.cassandraTable(DataLoader.Model.ks, DataLoader.Model.purchaseTable).cache().count()
    //VS
    spark.read.cassandraFormat(DataLoader.Model.purchaseTable, DataLoader.Model.ks).load().cache().count()
    </code></pre>

    <div class="canvasContainer"><canvas class="_canvas" data-name="space_efficiency"></canvas></div>

    <h3>Switching from Dataframe to RDD</h3>
    Switching from one to another has a cost. It'll change serialization from tungsten to RDD, and also break spark Dataframe lineage, preventing potential optimization.
    <pre><code>
        spark.createDataFrame(purchases, new StructType()
          .add(StructField("id", StringType, true))
          .add(StructField("val1", DoubleType, true))
          .add(StructField("val2", DoubleType, true)))
    </code></pre>

    <h3>Speed</h3>
    The <a href="https://github.com/QuentinAmbard/doc/blob/master/src/main/scala/BenchmarkDataframe.scala">following operations</a> have been tested against RDD, Dataframe and typed Dataset against a C* table of 200 000 000 rows. <br/>
    <div class="canvasContainerBig"><canvas class="_canvas" data-name="rdd_dataframe"></canvas></div>
    <div class="alert alert-info" role="alert">
        The read time isn't included in this benchmark and is an by an order of magnitude higher than the computation time
    </div>


    <h2>Cassandra Read & Write throughput - data locality</h2>
    <h3>General recommendations</h3>
    Here are a few advise to make sure you control your Read/Write throughput. Note that the default settings are usually good:
    <ul>
        <li>A custom connection factory (connection.factory=com.MyClass) can be used to increase the max number of connection and the query pool size</li>
        <li>Parameters like output.batch.grouping.buffer.size, output.batch.size.bytes, output.concurrent.writes can be tuned to increase the throughput, but the default values are usually good.</li>
        <li>The spark cassandra connector will try to group data per partition to increase write efficiency. A saveToCassandra operation will likely be faster on an ordered RDD. Do not order your data just for that reason. </li>
        <li>Unless it's a free operation, it's generally best not to focus too much on data locality.</li>
        <li>Spark will very likely hammer your cluster while reading/joining from a Cassandra table. In some cases, it can be worth limiting the maximum throughput of the connector for a specific operation. use input.reads_per_sec (with continuous paging only) or output.throughput_mb_per_sec.</li>
    </ul>

    <h3>Full table scan - RDD, mapper, Dataset or Dataframe?</h3>
    <h4>DSE continuous paging</h4>
    Continuous paging allows DSE server to continue to fetch data while spark is processing a page.
    See <a href="https://www.datastax.com/dev/blog/dse-continuous-paging-tuning-and-support-guide">DSE blog post for more information</a>
    It generally boosts cassandra read time and can be enabled with the following option:
    <pre><code>spark.dse.continuous_paging_enabled=true</code></pre>

    <h4>Full table scan with RDD and mapper:</h4>
    Cassandra connector provide a mapper allowing to automatically mount row to scala case classes. It's handy but comes at a cost and will add pressure on the gc.<br/>
    It might be preferable not to use the mapper on the hot path.<br/>
    <pre><code class="">
    spark.sparkContext.cassandraTable[Purchase](DataLoader.Model.ks, table).map(..).count()
    </code></pre>
    <h4>Full table scan with RDD:</h4>
    <pre><code>
    spark.sparkContext.cassandraTable(DataLoader.Model.ks, table).map(..).count()
    </code></pre>
    <h4>Full table scan with Dataset:</h4>
    <pre><code>
    spark.read.cassandraFormat(table, DataLoader.Model.ks).load().as[Purchase].map(..).count()
    </code></pre>
    <h4>Full table scan with Dataframe:</h4>
    <pre><code>
    spark.read.cassandraFormat(table, DataLoader.Model.ks).load().map(..).count()
    </code></pre>
    <div class="canvasContainer">
        <canvas class="_canvas" data-name="mapper_cost"></canvas>
    </div>
    <div class="alert alert-info" role="alert">
        Continuous paging clearly speeds up full scan. Make sure you <a href="https://www.datastax.com/dev/blog/dse-continuous-paging-tuning-and-support-guide">read this post before enabling it</a>.
    </div>
    <div class="alert alert-info" role="alert">
        RDD and DF are similar especially since we use a .map() operation (we just want to compare C* read time, not DF operation)
    </div>

    <div class="alert alert-warning" role="alert">
        The mapper comes at a cost and should be used wisely.
    </div>

    <h2>Pushdown filters</h2>
    Spark can push down filters to the source. If you perform a filter on a partition key, clustering column or token range, it can be pushed-down to Cassandra.<br/>
    .explain() can give you an idea of what's going on, but if you want to be sure a predicat is effectivly pushed-down to cassandra, change the following log level:
    <pre><code>
        Logger.getLogger("org.apache.spark.sql.cassandra").setLevel(Level.DEBUG)
    </code></pre>

    Search request can also be pushed down to DSE (this is not yet automatic in DSE 5.1).<br/>
    Search predicate allow instant count(*) operation, and are fast for queries retrieving a very small amount of data (<2-3%). It's usually faster to scan all the table after this limit.<br/>
    Enable search push-down the following parameter:

    <pre><code>
        //Globally: spark.sql.dse.solr.enable_optimization=true
        //Or locally per read
        val solrEnabledDataSet = spark.read
            .format("org.apache.spark.sql.cassandra")
            .options(Map(
            "keyspace" -> "ks",
            "table" -> "tab",
            "spark.sql.dse.solr.enable_optimization" -> "true")
            .load()
    </code></pre>
    Enable debug to make sure the filter is applied:
    <pre><code>
        Logger.getLogger("org.apache.spark.sql.SolrPredicateRules").setLevel(Level.DEBUG)
    </code></pre>
    <a href="https://docs.datastax.com/en/dse/5.1/dse-dev/datastax_enterprise/analytics/dseSearchAnalyticsOverview.html">Datastax search analytics integration</a>

    <h2>Shuffle</h2>
    Shuffle are expensive. Dataframe will try to optimize your lineage to ensure operations like reduces or filters are executed before the shuffle to reduce the amount of data.<br/>
    When working with RDDs, it's up to you to make those kind of optimization.<br/>
    This also includes reduce operations (if your final operation is to perform a reduce, always reduce before grouping).<br/>
    The number of object created during those operation shouldn't be too high. Always prefer mutable structures or Array[] to object instanciation in the hot path.<br>
    It's easy to detect shuffle in spark webUI (each new stage in the DAG is usually caused by a shuffle).<br/>
    The DF and RDD .explain() or .toDebugString can also be used:

    <pre><code>
        #RDD Join with shuffle (search for CoGrouped operations)
        (5) MapPartitionsRDD[12] at join at BenchmarkJoins.scala:126 []
         |  MapPartitionsRDD[11] at join at BenchmarkJoins.scala:126 []
         |  CoGroupedRDD[10] at join at BenchmarkJoins.scala:126 []
         +-(5) MapPartitionsRDD[7] at keyBy at BenchmarkJoins.scala:124 []
         |  |  CassandraTableScanRDD[6] at RDD at CassandraRDD.scala:19 []
         +-(5) MapPartitionsRDD[9] at keyBy at BenchmarkJoins.scala:125 []
            |  CassandraTableScanRDD[8] at RDD at CassandraRDD.scala:19 []

        #RDD Join without shuffle (search for CoGrouped operations)
        (5) MapPartitionsRDD[29] at join at BenchmarkJoins.scala:107 []
         |  MapPartitionsRDD[28] at join at BenchmarkJoins.scala:107 []
         |  CoGroupedRDD[27] at join at BenchmarkJoins.scala:107 []
         |  CassandraTableScanRDD[22] at RDD at CassandraRDD.scala:19 []
         |  CassandraTableScanRDD[26] at RDD at CassandraRDD.scala:19 []

        #Dataframe join
        == Physical Plan ==
        *BroadcastHashJoin [_1#116.user_id], [_2#117.user_id], Inner, BuildRight
        :- *Project [named_struct(user_id, user_id#103L, firstname, firstname#104, lastname, lastname#105, shop_id, shop_id#106L, title, title#107, zipcode, zipcode#108) AS _1#116]
        :  +- *Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@60c8909a [zipcode#108,shop_id#106L,title#107,user_id#103L,firstname#104,lastname#105] ReadSchema: struct&lt;_1:struct&lt;user_id:bigint,firstname:string,lastname:string,shop_id:bigint,title:string,zipc...
        +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, struct&lt;user_id:bigint,purchase_id:bigint,item:string,price:int&gt;, false].user_id))
           +- *Project [named_struct(user_id, user_id#94L, purchase_id, purchase_id#95L, item, item#96, price, price#97) AS _2#117]
              +- *Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@61503d00 [user_id#94L,purchase_id#95L,item#96,price#97] ReadSchema: struct&lt;_2:struct&lt;user_id:bigint,purchase_id:bigint,item:string,price:int&gt;&gt;

    </code></pre>
    <div class="alert alert-warning" role="alert">
        Avoid creating objects in the "hot path", it will overload the GC. Prefer mutable object / Array.
    </div>

    <h2>Joins</h2>
    <h3>Joining small dataset</h3>
    <h4>Dataframe</h4>
    Spark usually pick a SortMergeJoin to perform joins. However, if the data being joined is small enough, it'll use a broadcast join.<br/>
    By doing so it'll first fetch the data and then send it once to each executor, saving unecessary calls to Cassandra. The threshold is delimited by
    <pre><code>spark.sql.autoBroadcastJoinThreshold = 10M</code></pre>

    <pre><code>
      val shop = spark.read.cassandraFormat(DataLoader.Model.shopTable, DataLoader.Model.ks).load()
      val users = spark.read.cassandraFormat(userTable, DataLoader.Model.ks).load()
      //Force a broadcast join, will override conf threshold. Current limit to 2GB (block size, SPARK-6235)
      import org.apache.spark.sql.functions.broadcast
      val join = users.join(broadcast(shop), "shop_id")
    </code></pre>

    <h4>RDD</h4>
    The same logic can be applied to RDD. The small RDD must first be collected and then broadcasted:
    <pre><code>
      //Start by collecting the small RDD and broadcast it:
      val shops = spark.sparkContext.cassandraTable[Shop](DataLoader.Model.ks, shopTable).keyBy(s => s.shop_id).collect().toMap
      val broadcastShops = spark.sparkContext.broadcast(shops)
      val users = spark.sparkContext.cassandraTable[User](DataLoader.Model.ks, userTable)
      val join: RDD[(User, Shop)] = users.flatMap(user => {
        broadcastShops.value.get(user.shop_id).map {shop => (user, shop)}
      })
    </code></pre>
    <div class="container">
        <div class="row">
            <div class="col-sm">
                <div class="canvasContainer"><canvas class="_canvas" data-name="broadcast_join"></canvas></div>
            </div>
            <div class="col-sm">
                <div class="canvasContainer"><canvas class="_canvas" data-name="broadcast_join_10"></canvas></div>
            </div>
        </div>
    </div>


    <h3>Normal joins</h3>
    <h4>RDD joins</h4>
    Joins can be costly. If the 2 RDD being joins aren't sharing the same partitioner, a shuffle will be require to distribute the key to the same partitions.<br/>
    To prevent that, a common partitioner can be used. Partitioner aren't preserved by any transformation operation.<br/>

    <br/>
    To join 1 RDD with a non-cassandra RDD, the connector allows you to use .applyPartitionerFrom(otherRDD). All Partition Keys columns must also be present in the keys of the target RDD.
    <h4>Dataframe joins</h4>
    As of DSE 5.1, Dataframe aren't aware of partitioner. As result, the RDD joins optimization can't be applied (data will be shuffled even if they are partitioned on a common join key).<br/>

    <h4>Join with a cassandra table</h4>
    The RDD joinWithCassandra operation will trigger 1 query for each RDD row and won't trigger shuffle.<br/>
    DatafFrame join will first perform 2 full table scan, and then perform a join on those 2 Dataframe, which is likely to be less efficient. Spark SQL performs the same way.<br/>
    This has a very high cost especially when joins are executed against a small percentage of your table. <br/>
    If you want to join 10 rows against a table having 1M entries, spark will start by reading 1M rows and then shuffle to only keep 10 rows.<br/>

    Depending of the join cardinality, it's generally faster to perform a Full Table Scan of the biggest table and then perform a join against the smaller table.<br/>
    The following snippets are different joins technique between Users and Purchases. <br/>
    In these examples, 1 user has 10 to 20 purchases, and we want to join users and purchases (RDD[User, Purchase] or RDD[User, Seq[Purchase]]).
    //TODO: 1 vs 10 or 10 vs 1
    <h5>joinWithCassandraTable</h5>
    <pre><code>
    spark.sparkContext.cassandraTable(DataLoader.Model.ks, userTable).joinWithCassandraTable(DataLoader.Model.ks, purchaseTable)
    </code></pre>
    <h5>joinWithCassandraTable with mapper</h5>
    <pre><code>
    //mapper comes at a cost
    spark.sparkContext.cassandraTable[User](DataLoader.Model.ks, userTable).joinWithCassandraTable[Purchase](DataLoader.Model.ks, purchaseTable)
    </code></pre>
    <h5>RDD join keyBy</h5>
    <pre><code>
    //Inefficient, will trigger a shuffle, don't do that.
    val purchases = spark.sparkContext.cassandraTable(DataLoader.Model.ks, purchaseTable).keyBy(p => p.user_id)
    val users = spark.sparkContext.cassandraTable(DataLoader.Model.ks, userTable).keyBy(u => u.user_id)
    users.join(purchases)
    </code></pre>
    <h5>RDD join keyBy same partitioner</h5>
    <pre><code>
    //2 Full table scans and then 1 join without shuffle. Requires an object (Tuple1), Usually less efficient.
      val purchases = spark.sparkContext.cassandraTable(DataLoader.Model.ks, purchaseTable).keyBy[Tuple1[Long]]("user_id")
      val users = spark.sparkContext.cassandraTable(DataLoader.Model.ks, userTable).keyBy[Tuple1[Long]]("user_id").applyPartitionerFrom(purchases)
      users.join(purchases)
    </code></pre>
    <h5>RDD join span by key</h5>
    <pre><code>
    //Inefficient, will trigger a shuffle, don't do that.
      val purchases = spark.sparkContext.cassandraTable(DataLoader.Model.ks, purchaseTable).spanBy(p => p.user_id)
      val users = spark.sparkContext.cassandraTable(DataLoader.Model.ks, userTable).keyBy(u => u.user_id)
      val joinrdd = purchases.join(users)
    </code></pre>
    <h5>joinWithCassandraTable span by key</h5>
    <pre><code>
      spark.sparkContext.cassandraTable(DataLoader.Model.ks, purchaseTable).spanBy(r => r.getLong("user_id")).joinWithCassandraTable(DataLoader.Model.ks, userTable)
    </code></pre>
    <h5>Dataframe joins</h5>
    <pre><code>
    val purchases = spark.read.cassandraFormat(purchaseTable, DataLoader.Model.ks).load()
    val users = spark.read.cassandraFormat(userTable, DataLoader.Model.ks).load()
    val joinDF = users.join(purchases, purchases("user_id") === users("user_id"))
    </code></pre>
    <h5>Dataset joins</h5>
    <pre><code>
    val purchases = spark.read.cassandraFormat(purchaseTable, DataLoader.Model.ks).load().as[Purchase]
    val users = spark.read.cassandraFormat(userTable, DataLoader.Model.ks).load().as[User]
    val joinDS = users.join(purchases, purchases("user_id") === users("user_id"))
    </code></pre>
    <a href="https://github.com/QuentinAmbard/doc/blob/master/src/main/scala/BenchmarkJoins.scala">Full code</a><br/>
    Result with a 5 nodes cluster, RF 3, 1 table with 1 000 000 users with and avg of 10 purchases,  (purchase table with 10 000 000 row, full table scan of both users and purchases)
    <div class="canvasContainerBig"><canvas class="_canvas" data-name="join_cardinality"></canvas></div>
    Result with a 5 nodes cluster, RF 3, 1 table with 100000 users joining with and avg of 10 purchases  (purchase table with 10 000 000 row, select only 10% of the table)
    <div class="canvasContainerBig"><canvas class="_canvas" data-name="join_cardinality_10"></canvas></div>
    Continuous paging on: Result with a 5 nodes cluster, RF 3, 1 table with 1 000 000 users with and avgiv>
    <div class="alert alert-info" role="alert">
        There is no difference here between mapper and non-mapper scan because we only count the number of rows. It would have been worse if a .map() operation was triggered.
    </div>

    <h2>Dataframe vs SparkSQL</h2>
    SparkSQL is easy to read and will result in the same operation as standar Dataframe operation. UDF can also be registered to be accessible from a SparkSQL query.<br/>

    <pre><code>
    df.createOrReplaceTempView("myTable")
    spark.sqlContext.udf.register("strLen", (s: String) => s.length())

    </code></pre>

    <h2>Spark asynchronous process</h2>
    Every row inside each spark core is processed in sequential order.<br/>
    To increase parralellism, we usually increase the number of core in each executor.<br/>
    However some long operation (query an external database or webervice) could benefit to be run asynchronously.<br/>
    The following <a href="https://github.com/QuentinAmbard/doc/blob/master/src/main/scala/AsynchRDDHelper.scala">AsynchRDDHelper.scala</a> can be used as inspiration.<br/>
    Exemple of an asynchronous .map():
    <pre><code>
    val value: RDD[String] = myRDD.mapPartitionAsynch(partition => {
      partition.mapAsync(row => {
        myService.slowExternalOperation(row)
      })
    })
    </code></pre>
    <a href="http://www.russellspitzer.com/2017/02/27/Concurrency-In-Spark/">Russel Concurrency blog post</a>


    <h2>Integration tests & unit testing</h2>
    The C* table won't be saved in the metastore if the OSS spark is used inside unit tests.<br/>
    The best way to solve this issue is to manually register them when the session is started:
    <pre><code>
        class SparkTest extends FlatSpec with BeforeAndAfterAll {

          override def beforeAll(): Unit = {
            super.beforeAll()
            spark = SparkSession.builder
              .master("local[3]")
              .appName("Datastax Scala example")
              .config("spark.cassandra.connection.host", "127.0.0.1")
              .config("spark.cassandra.auth.username", "cassandra")
              .config("spark.cassandra.auth.password", "cassandra")
              .getOrCreate()

            //Register all C* table in spark catalog
            val keyspace = "keyspae"
            spark.sql(s"create database if not exists $keyspace")
            val tables = CassandraConnector(spark.sparkContext.getConf).withSessionDo(session => {
              import scala.collection.JavaConversions._
              session.getCluster.getMetadata.getKeyspace(keyspace).getTables.map(_.getName)
            })
            tables.foreach(tableName => {
              spark.sql(s"drop table IF EXISTS $keyspace.$tableName")
              spark.sql(s"create table IF NOT EXISTS $keyspace.$tableName USING org.apache.spark.sql.cassandra OPTIONS (table='$tableName', keyspace='$keyspace')")
            })
            //or run dse client-tool configuration byos-export configuration.properties
            //add the options to your spark configuration
            //And include in your CP the byos jar: (cp /opt/dse/dse-6.0.4/resources/spark/client-lib/dse-byos_2.11-6.0.4.jar .)

            //For unit tests (no C*), you should not hit any C* table, and you should manually generate spark table from a local file instead.
            //For example to generate some json data for the unit test using existing C* row
            //(do it only once to generate some json data)
            //spark.sql("select * from keyspace.my_table").coalesce(1).write.json("my_table_test_data.json")

            //And then instead of having to start a C* server, create a C* session and register the table, you can register the json file:
            spark.sql(s"create table IF NOT EXISTS keyspace.my_table USING json LOCATION 'bets_test_data.json' ")

          }

          override def afterAll {
            spark.close()
          }

          "Some test" should "display bet content" in {
            val data = spark.sql("select * from keyspace.my_table").collect()
            val results = myservice.transformData(data)
            //Then do usual tests: assert results === xxxx
          }

        }

    </code></pre>

    <h2>Object singleton</h2>
    Opening resources to an external connection is often expensive and slow. <br/>
    As multiples tasks can run in parrallel in the same JVM, a classic synchronized singleton approach can be used to make sure only 1 instance is created (the holder should be called only once per partition, not per row).<br/>
    Example:
    <pre><code>
    object MyConnectionlHolder {
        val lock = new Object()
        var connection: Option[MyConnection] = None
        def getInstance(poolSize: Int) = {
            lock.synchronized {
                if (pool.isEmpty) {
                    val connection = createMyConnection()
                    pool = Some(connection)
                }
                pool.get
            }
        }
    }
    </code></pre>

    <h2>Dependency injection</h2>
    Do you really need that in your Spark job? It's often easier to manually build the classes/dependancies.<br/>
    If no scala-native solution work for you, check <a href="https://github.com/adamw/macwire">MacWire</a>

    <h2>Cache/persist vs Checkpoint</h2>
    Use .cache() to avoid re-computing your DAG twice when it forks.<br/>
    Used checkpoint() in long and complex to avoid having to recompute all the lineage if a stage fails.


    <h2>Packaging</h2>
    Datastax libraries are added to the JVM classpath during any spark job launch.<br/>
    All these lib (including spark-cassandra-connector, netty, guava etc.) MUST NOT be shipped in your spark applications. There are plenty libraries and it's almost impossible to manually keep track of all versions.<br/>
    If you really need a specific version of those library, you'll have to shade it.<br/>
    Datastax provides a maven repository to directly include those libraries in your project.<br/>
    This dependency is required at runtime while working/testing in your IDEA, but musn't be shipped. Maven profiles are usually leveraged to switch the scope from provided to compile.  <br/>
    Repository: <a href="repo.datastax.com/public-repos/com/datastax/dse/dse-spark-dependencies/">repo.datastax.com/public-repos/com/datastax/dse/dse-spark-dependencies/</a>
    <pre><code>
        &lt;dependency&gt;
            &lt;groupId&gt;com.datastax.dse&lt;/groupId&gt;
            &lt;artifactId&gt;dse-spark-dependencies&lt;/artifactId&gt;
            &lt;version&gt;5.1.7&lt;/version&gt;
            &lt;scope&gt;${dse.scope}&lt;/scope&gt;
        &lt;/dependency&gt;

        ...

        &lt;profiles&gt;
            &lt;profile&gt;
                &lt;id&gt;dev&lt;/id&gt;
                &lt;activation&gt;
                  &lt;activeByDefault&gt;true&lt;/activeByDefault&gt;
                &lt;/activation&gt;
                &lt;properties&gt;
                &lt;dse.scope&gt;compile&lt;/dse.scope&gt;
              &lt;/properties&gt;
            &lt;/profile&gt;
            &lt;profile&gt;
                &lt;id&gt;build&lt;/id&gt;
                &lt;properties&gt;
                &lt;dse.scope&gt;provided&lt;/dse.scope&gt;
              &lt;/properties&gt;
            &lt;/profile&gt;
        &lt;/profiles&gt;

        //maven clean install -P build
    </code></pre>
    <a href="https://www.datastax.com/dev/blog/spark-application-dependency-management">More detail from Datastax blog post</a>

    <h2>Consistency</h2>
    Keep in mind that the default WRITE consistency level is LOCAL_QUORUM, but the default READ consistency consistency is LOCAL_ONE.<br/>
    If you want to use a sparkSQL query to ensure a LOCAL_QUORUM update is successful, switch input.consistency.level to LOCAL_QUORUM while starting a dse sql sesssion.

    <h2>Thrift server</h2>
    DSE includes a thrift server, creating 1 spark context ready to execute SparkSQL queries. It's easy to plug any SQL sources over your Cassandra tables using the ODBC or JDBC driver.<br/>
    If your queries fail due to Out Of Memory errors, change the following settings:
    <ul>
        <li>Always LIMIT the number of result in your query</li>
        <li>Select the minimum field required</li>
        <li>Limit max data returned to the driver: spark.driver.maxResultSize=200m. <br/>The spark job/query will fail if more than 200m are returned to the driver (will happen for queries without limit).</li>
        <li>Increase driver (thrift server) memory: --conf spark.driver.memory=2g. <br/> Should be >> spark.driver.maxResultSize. (1GB or 2GB is usually enough, could be more if a fair scheduler is used with multiple spark requests running in parallel)</li>
        <li>Increase executor memory: --conf spark.executor.memory=4g. Should be at least 4GB. If multiple cores are used per executor, each task will roughly have (spark.executor.memory / numberOfCorePerExecutor / 2) RAM available for each spark partition.</li>
        <li>Make sure your number of cores is in phase with the memory assigned to each executor: --conf spark.cores.max=6.</li>
        <li>Reduce spark partition size (of the first request): --conf spark.cassandra.input.split.size_in_mb=128</li>
        <li>Ultimatly, incrementalCollect could be enabled: --conf spark.sql.thriftServer.incrementalCollect=true. Could have performances penalty, avoid if possible</li>
    </ul>

    <pre><code>
    dse spark-sql-thriftserver start --conf spark.driver.memory=2g --conf spark.cores.max=6 --conf spark.executor.memory=5g --conf spark.cassandra.input.split.size_in_mb=128 --conf spark.driver.maxResultSize=200m --conf spark.sql.thriftServer.incrementalCollect=false
    #test connection with beeline:
    dse beeline
    !connect jdbc:hive2://localhost:10001
    select count(*) from ks.mytable;
    </code></pre>

    <a href="http://www.russellspitzer.com/2017/05/19/Spark-Sql-Thriftserver/">More details on Russell's blog post</a>

    <h2>Spark streaming guide for kafka</h2>
    General advise
    <ul>
        <li>If you are reading from kafka and have a high number of kafka partition, it's probably a good idea to start your microbatches with a coaelesce</li>
        <li>If your streaming job gets too complicated with many small steps and many .cache() operations, your throughput might become extremly low.<br/>
            In this case, it's sometime better to reduce the number of steps and stop developping using "spark-logic".<br/>
            Instead, in a single .map() operation, you can create a <a href="https://github.com/QuentinAmbard/doc/blob/master/src/main/scala/AsynchRDDHelper.scala">multithreaded operation </a>processing line by line as you would do in a normal batch.<br/>
            It's often faster, easier to reason and maintain, especially when you start having tens of final operations with very large DAGS.
        </li>
    </ul>
    <h3>Before structured streaming</h3>
    <ul>
        <li>Don't use spark native checkpointing. It's not safe, messages can be lost.</li>
        <li>Use a lightweight transaction to save your checkpoints to ensure you don't loose any data and a single app is running. </li>
        <li>To make sure you don't loose any data, you need to fail as soon as you get an error. If you don't do so, spark will skip the microbatch and process the next one.<br/>
            By doing so, keep in mind that an applicative error (ex: NPE in your code) will stuck your streaming app since it will retry forever. You might want to cache all a: //TODO add code here</li>
    </ul>
    <h3>Exactly once</h3>
    Want to make sure you don't miss any message? Make sure you read this presentation:
    <a href="https://www.dropbox.com/s/kbb87gad4mmy1iq/exactly-once-final.pptx?dl=0">Exactly once, or At Least Once + Idempotence</a>
    <h3>Structured streaming</h3>
    <a href="https://jaceklaskowski.gitbooks.io/spark-structured-streaming/">Jacek Laskowski notebook</a>
    <ul>
        <li>Still need to make sure 2 applications with the same name aren't checkpointing at the same time. //TODO: what can we do?</li>
        <li>Structured streaming finally fail safe</li>
        <li>You can use a custom sink to mimic old streaming behavior (microbatch)</li>
        <li>Checkpointing requires many sequential operation and can be slow on distributed filesystem like DSEFS. Any custom HadoopFilesystem can be used to directly save data to C*. //TODO: add link</li>
    </ul>


    <h2>Extra resources</h2>
    <a href="https://www.gitbook.com/book/umbertogriffo/apache-spark-best-practices-and-tuning/details">https://www.gitbook.com/book/umbertogriffo/apache-spark-best-practices-and-tuning/details</a>
    <br/>
    <br/>
    <br/>
    <br/>
    <em>Disclaimer: this guide is a collection of personal notes, without warrenty of any kind. It does not replace Spark official documentation and/or Datastax official recommendation.<br/>
        For official recommendations, always refer to official documentation and/or ask official support.<br/>
    THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
    </em>

    <br/>
    <br/>
    <br/>
    <br/>

 </div>